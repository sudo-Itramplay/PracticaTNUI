{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kCuUg8CeVCjU",
   "metadata": {
    "id": "kCuUg8CeVCjU"
   },
   "source": [
    "# **Pr√†ctica 3: n-Grames Oberts i Classificador Ingenu de Bayes**\n",
    "\n",
    "Aquest exercici explora els **n-Grames Oberts** com a **representaci√≥ de paraules**, demostrant la seva robustesa davant del soroll i del desordre.\n",
    "\n",
    "Definirem un **$n$-grama** com una configuraci√≥ espec√≠fica de $n$ lletres consecutives dins d'una paraula.\n",
    "\n",
    "+ Per exemple, `at` i `i√≥` s√≥n alguns dels bigrames ($2$-grames) que formen part de la paraula `atenci√≥`. Fent servir aquest concepte, podriem dir que la paraula `atenci√≥` es pot representar pel conjunt de bigrames `{at, te, en, nc, ci, i√≥}`.\n",
    "\n",
    "Un **$n$-grama obert** √©s una configuraci√≥ que defineix un subconjunt ordenat ‚Äîper√≤ no necess√†riament contigu‚Äî de $n$ lletres dins d'una paraula.\n",
    "\n",
    "+ Per exemple, si considerem la paraula `hello`, el conjunt de bigrames oberts √©s `{he, hl, ho, el, eo, ll, lo}`.\n",
    "+ En moltes ocasions tamb√© √©s √∫til considerar el comen√ßament i el final d‚Äôuna paraula com a casos especials, tenint en compte l‚Äôespai en blanc.\n",
    "En aquest cas, per exemple, la paraula `hello` genera el seg√ºent conjunt ampliat de bigrames oberts:\n",
    "`{ _h, he, hl, ho, el, eo, ll, lo, o_ }`.\n",
    "\n",
    "> El cervell hum√† pot llegir frases amb les lletres internes desordenades sempre que: La primera i l‚Äô√∫ltima lletra de cada paraula es mantinguin al lloc, la paraula tingui una longitud suficient, el context de la frase sigui clar. Aix√≤ passa perqu√®, quan llegim, no analitzem cada lletra una per una, sin√≥ que reconeixem les paraules com a formes globals i utilitzem el context sem√†ntic per omplir els buits. El cervell fa una mena de ‚Äúcorrecci√≥ autom√†tica‚Äù basant-se en les paraules que espera veure.\n",
    "\n",
    "> Exemple: `Segns un etsdui de la Uinveristtat de Cmarbigde, el crevell pot lgegir paaulebs ambl les lleterres barajades i mab srrol sense mases dfiicultats.`\n",
    "\n",
    "> Els n-grames oberts (open n-grams) donen una explicaci√≥ molt natural de per qu√® podem llegir/reconeixer paraules amb les lletres internes desordenades.\n",
    "\n",
    "La definici√≥ dels $n$-grames oberts es basa en dos aspectes clau:\n",
    "+ Ordenaci√≥: Les lletres han d‚Äôapar√®ixer en el mateix ordre que en la paraula original (per exemple, `NBK` √©s un 3-grama obert v√†lid de la paraula `NOTEBOOK`, per√≤ `KBN` no ho √©s, perqu√® la $\\text{K}$ apareix despr√©s de la $\\text{B}$ i la $\\text{N}$ a la paraula).\n",
    "+ No contig√ºitat: Les lletres poden estar separades per qualsevol nombre d‚Äôaltres lletres (aix√≤ √©s el que fa que la representaci√≥ sigui robusta davant de soroll, com ara errors tipogr√†fics o lletres que falten).\n",
    "\n",
    "El nombre total de $n$-grames oberts per a una paraula de $L$ lletres ve donat pel coeficient binomial $\\binom{L}{n}$.\n",
    "\n",
    "Per a `NOTEBOOK` ($L=8$ i $n=3$), el total √©s:\n",
    "\n",
    "$$\\binom{8}{3} = \\frac{8!}{3!(8-3)!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = \\mathbf{56}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJ03cE5ibYSI",
   "metadata": {
    "id": "UJ03cE5ibYSI"
   },
   "source": [
    "## Part 1: Extracci√≥ de Caracter√≠stiques ‚Äî $N$-grames oberts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iems6AUItrER",
   "metadata": {
    "id": "iems6AUItrER"
   },
   "source": [
    "### Tasca 1\n",
    "\n",
    "Implementa una per extreure els $n$-grames oberts d‚Äôuna paraula, que ens servir√† com un nou tipus de representaci√≥ de les paraules d'un text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "CsYW1ay_emS4",
   "metadata": {
    "id": "CsYW1ay_emS4"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_open_ngrams(word: str, n: int, include_boundaries: bool = True) -> set:\n",
    "    \"\"\"\n",
    "    Genera un conjunt d‚Äôn-grams oberts per a una paraula donada, amb un tractament espec√≠fic dels l√≠mits.\n",
    "\n",
    "    Args:\n",
    "        - word (str): La paraula d‚Äôentrada.\n",
    "        - n (int): L‚Äôordre de l‚Äôn-gram (per exemple, 2 per a bigrames, 3 per a trigrames).\n",
    "        - include_boundaries (bool): Si √©s True, afegeix un gui√≥ baix '_' al principi i al\n",
    "                                final de la paraula per incloure els l√≠mits inicial/final.\n",
    "\n",
    "    Returns:\n",
    "        - set: Un conjunt d‚Äôn-grams oberts √∫nics.\n",
    "    \"\"\"\n",
    "    \n",
    "    open_ngrams = set()\n",
    "\n",
    "    # 1. Generar N-grams sense boundaries (Core)\n",
    "    # combinations retorna tuples, fem servir join per convertir-les a string\n",
    "    if len(word) >= n:\n",
    "        for ngram_tuple in combinations(word, n):\n",
    "            open_ngrams.add(\"\".join(ngram_tuple))\n",
    "\n",
    "    # 2. Generar boundaries (si es demana)\n",
    "    if include_boundaries:\n",
    "        \n",
    "        # Cas especial: n == 1\n",
    "        # Simplement afegim el marcador de l√≠mit \"_\"\n",
    "        if n == 1:\n",
    "            open_ngrams.add(\"_\")\n",
    "        \n",
    "        # Casos generals (n >= 2)\n",
    "        else:\n",
    "            # Regla: '_' + primer car√†cter + (n-2 car√†cters de la resta)\n",
    "            # Aix√≤ assegura que el l√≠mit nom√©s toca el primer car√†cter real.\n",
    "            first_char = word[0]\n",
    "            rest_of_word = word[1:]\n",
    "            \n",
    "            # Necessitem triar (n-2) car√†cters de la resta de la paraula\n",
    "            # Si n=2, triem 0 car√†cters (el bucle s'executa una vegada amb string buit)\n",
    "            if len(rest_of_word) >= (n - 2):\n",
    "                for sub_combo in combinations(rest_of_word, n - 2):\n",
    "                    ngram = \"_\" + first_char + \"\".join(sub_combo)\n",
    "                    open_ngrams.add(ngram)\n",
    "\n",
    "            # Regla: (n-2 car√†cters de l'inici fins al pen√∫ltim) + √∫ltim car√†cter + '_'\n",
    "            last_char = word[-1]\n",
    "            start_of_word = word[:-1]\n",
    "            \n",
    "            if len(start_of_word) >= (n - 2):\n",
    "                for sub_combo in combinations(start_of_word, n - 2):\n",
    "                    ngram = \"\".join(sub_combo) + last_char + \"_\"\n",
    "                    open_ngrams.add(ngram)\n",
    "\n",
    "    return open_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440894c1",
   "metadata": {
    "id": "440894c1"
   },
   "source": [
    "Executa el test de la funci√≥ `get_open_ngrams` amb la paraula `hello` i comprova que funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e11350",
   "metadata": {
    "id": "71e11350"
   },
   "outputs": [],
   "source": [
    "word_to_test = \"hello\"\n",
    "n_gram_order_test = 2\n",
    "\n",
    "ngrams_with_boundaries = get_open_ngrams(word_to_test, n_gram_order_test, include_boundaries=True)\n",
    "\n",
    "assert ngrams_with_boundaries == {'_h', 'el', 'eo', 'he', 'hl', 'ho', 'll', 'lo', 'o_'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64aac058-203a-4f82-a1b3-1466af7f8102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h', 'e')\n",
      "('h', 'l')\n",
      "('h', 'l')\n",
      "('h', 'o')\n",
      "('e', 'l')\n",
      "('e', 'l')\n",
      "('e', 'o')\n",
      "('l', 'l')\n",
      "('l', 'o')\n",
      "('l', 'o')\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "word = \"hello\"\n",
    "L = len(word)\n",
    "for i in combinations(\"hello\",2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lrk-wL3iVCjW",
   "metadata": {
    "id": "lrk-wL3iVCjW"
   },
   "source": [
    "## Part 2: Col¬∑lisions entre paraules\n",
    "\n",
    "Podem avaluar la capacitat per representar paraules comprovant la **taxa de col¬∑lisi√≥ de paraules**, que es produeix quan dues paraules diferents tenen el mateix conjunt de caracter√≠stiques.\n",
    "\n",
    "Per fer-ho farem servir el fitxer `dataset.csv`, que cont√© frases en diferents idiomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "QUTCJQ9zklgm",
   "metadata": {
    "id": "QUTCJQ9zklgm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitxer carregat.\n",
      "Nombre de frases: 22000\n",
      "Exemple de frase: klement gottwaldi surnukeha palsameeriti ning paigutati mausoleumi surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundem√§rke  aastal viidi ta surnukeha mausoleumist √§ra ja kremeeriti zl√≠ni linn kandis aastatel ‚Äì nime gottwaldov ukrainas harkivi oblastis kandis zmiivi linn aastatel ‚Äì nime gotvald\n",
      "Exemple d'etiqueta: Estonian\n",
      "\n",
      "Distribuci√≥ de Llengues:\n",
      "Counter({'Estonian': 1000, 'Swedish': 1000, 'Thai': 1000, 'Tamil': 1000, 'Dutch': 1000, 'Japanese': 1000, 'Turkish': 1000, 'Latin': 1000, 'Urdu': 1000, 'Indonesian': 1000, 'Portugese': 1000, 'French': 1000, 'Chinese': 1000, 'Korean': 1000, 'Hindi': 1000, 'Spanish': 1000, 'Pushto': 1000, 'Persian': 1000, 'Romanian': 1000, 'Russian': 1000, 'English': 1000, 'Arabic': 1000})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "dataset_file_path = 'dataset.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(dataset_file_path)\n",
    "\n",
    "    # Extracci√≥ de textes i etiquetes\n",
    "    texts = df['Text'].tolist()\n",
    "    labels = df['language'].tolist()\n",
    "\n",
    "    print(\"Fitxer carregat.\")\n",
    "    print(f\"Nombre de frases: {len(texts)}\")\n",
    "    print(f\"Exemple de frase: {texts[0]}\")\n",
    "    print(f\"Exemple d'etiqueta: {labels[0]}\")\n",
    "    print(\"\\nDistribuci√≥ de Llengues:\")\n",
    "    print(Counter(labels))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Fitxer '{dataset_file_path}' no trobat.\")\n",
    "    print(\"Assegura't que dataset.csv est√† accessible.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al carregar el fitxer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tZWkJiSfrC3_",
   "metadata": {
    "id": "tZWkJiSfrC3_"
   },
   "source": [
    "### Tasca 2\n",
    "\n",
    "Escriu un codi en Python que:\n",
    "\n",
    "1. Extreu totes les paraules √∫niques de `texts` i posa-les a un conjunt que es dir√† `unique_words`\n",
    "2. Genera, per a cada paraula, els bigrames oberts ($n$=2) i conta quants conjunts de bigrames √∫nics hi ha al dataset.\n",
    "3. Detecta si hi ha col¬∑lisions al conjunt de dades, √©s a dir, casos en qu√® diferents paraules tenen exactament el mateix conjunt d‚Äô$n$-grames.\n",
    "4. Imprimeix:\n",
    "    + El total de paraules **√∫niques** extretes.\n",
    "    + El total de conjunts **√∫nics** d‚Äôn-grames generats.\n",
    "    + El nombre de paraules √∫niques implicades en col¬∑lisions.\n",
    "    + 5 exemples de parellles de paraules en col¬∑lisi√≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "new_collision_code",
   "metadata": {
    "id": "new_collision_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paraules √∫niques extretes: 278630\n",
      "Total conjunts √∫nics d'n-grames: 277959\n",
      "Nombre de paraules implicades en col¬∑lisions: 1335\n",
      "Percentatge de col¬∑lisi√≥: 0.48%\n",
      "\n",
      "5 exemples de grups de paraules en col¬∑lisi√≥:\n",
      "  1. ['riikliku', 'riiklikku'] -> Comparteixen els mateixos bigrames\n",
      "  2. ['poliitika', 'politika'] -> Comparteixen els mateixos bigrames\n",
      "  3. ['tehnicii', 'tehnici'] -> Comparteixen els mateixos bigrames\n",
      "  4. ['ŸÜ⁄©ŸÜÿØ', 'ŸÜ⁄©ŸÜŸÜÿØ'] -> Comparteixen els mateixos bigrames\n",
      "  5. ['filosof', 'filosoof'] -> Comparteixen els mateixos bigrames\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Extreu totes les paraules √∫niques de 'texts'\n",
    "unique_words = set()\n",
    "\n",
    "for text in texts:\n",
    "    if isinstance(text, str): # Verificaci√≥ de seguretat per si hi ha NaNs\n",
    "        # Utilitzem regex per trobar paraules (\\w+), convertim a min√∫scules\n",
    "        # per evitar que \"Hola\" i \"hola\" siguin diferents.\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        unique_words.update(words)\n",
    "\n",
    "# 2. Genera els bigrames oberts (n=2) amb l√≠mits per a cada paraula √∫nica\n",
    "# Utilitzem un diccionari on:\n",
    "# Clau: Una representaci√≥ immutable del conjunt d'n-grams\n",
    "# Valor: Llista de paraules que generen aquest conjunt\n",
    "ngram_signature_map = defaultdict(list)\n",
    "\n",
    "for word in unique_words:\n",
    "    # Generem el set\n",
    "    ngrams_set = get_open_ngrams(word, n=2, include_boundaries=True)\n",
    "    \n",
    "    # Convertim el set a una tupla ordenada per poder usar-la com a clau de diccionari (hashable)\n",
    "    ngrams_signature = tuple(sorted(list(ngrams_set)))\n",
    "    \n",
    "    # Guardem la paraula sota aquesta signatura\n",
    "    ngram_signature_map[ngrams_signature].append(word)\n",
    "\n",
    "# 3. Identifica i compta les col¬∑lisions\n",
    "# Una col¬∑lisi√≥ existeix si la llista de paraules per a una clau t√© longitud > 1\n",
    "collision_groups = []\n",
    "words_in_collision_count = 0\n",
    "\n",
    "for signature, words_list in ngram_signature_map.items():\n",
    "    if len(words_list) > 1:\n",
    "        collision_groups.append(words_list)\n",
    "        words_in_collision_count += len(words_list)\n",
    "\n",
    "# 4. Compta i imprimeix resultats\n",
    "total_unique_words = len(unique_words)\n",
    "total_unique_ngram_sets = len(ngram_signature_map)\n",
    "\n",
    "\n",
    "print(f\"Total paraules √∫niques extretes: {total_unique_words}\")\n",
    "print(f\"Total conjunts √∫nics d'n-grames: {total_unique_ngram_sets}\")\n",
    "print(f\"Nombre de paraules implicades en col¬∑lisions: {words_in_collision_count}\")\n",
    "\n",
    "# 5. C√†lcul del percentatge de col¬∑lisi√≥\n",
    "if total_unique_words > 0:\n",
    "    percentatge = (words_in_collision_count / total_unique_words) * 100\n",
    "    print(f\"Percentatge de col¬∑lisi√≥: {percentatge:.2f}%\")\n",
    "\n",
    "# Mostrem els primers 5 trobats\n",
    "print(\"\\n5 exemples de grups de paraules en col¬∑lisi√≥:\")\n",
    "for i, group in enumerate(collision_groups[:5]):\n",
    "    print(f\"  {i+1}. {group} -> Comparteixen els mateixos bigrames\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcrk6JPFnn2E",
   "metadata": {
    "id": "rcrk6JPFnn2E"
   },
   "source": [
    "## Part 2: Classificador Ingenu de Bayes\n",
    "\n",
    "L'objectiu √©s fer un classificador que sigui capa√ß de predir la llengua d'una frase a partir de representar la frase com el conjunt de bigrames de les seves paraules.\n",
    "\n",
    "Per fer-ho comen√ßarem dividint el nostre dataset en una part de *training* i una de *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ed047c",
   "metadata": {
    "id": "c8ed047c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts length: 17600\n",
      "Test texts length: 4400\n",
      "Training labels length: 17600\n",
      "Test labels length: 4400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training texts length: {len(X_train)}\")\n",
    "print(f\"Test texts length: {len(X_test)}\")\n",
    "print(f\"Training labels length: {len(y_train)}\")\n",
    "print(f\"Test labels length: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6452721",
   "metadata": {
    "id": "d6452721"
   },
   "source": [
    "### Tasca 3: Preparaci√≥ de les dades\n",
    "1. Extreu els bigrames oberts de cada un dels textos del conjunt d'entrenament (els bigrames oberts d'un text √©s el resultat de la uni√≥ dels conjunts de bigrames de totes les seves paraules).\n",
    "2. Un cop calculats, representa cada text d'entrenament amb una llista (`X_train_ngrams[:][:]`) de bigrames enlloc d'un conjunt. El primer √≠ndex √©s l'√≠ndex del text i el segon la llista dels seus bigrames.\n",
    "3. Mostra la llista que correspon al primer text d'entrenament.\n",
    "\n",
    "Indicaci√≥: Per extreure totes les paraules d'un text pots fer servir aquest codi:\n",
    "\n",
    "```python\n",
    "text_lower = text.lower()\n",
    "words_in_text = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced7bf3c-6e27-4a5c-a2b8-26eeec386eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_open_bigrams(text):\n",
    "    \"\"\"\n",
    "    Extreu els bigrames oberts (open bigrams) d‚Äôun text.\n",
    "\n",
    "    Args:\n",
    "        - text (str): Text d‚Äôentrada del qual es volen extreure els bigrames.\n",
    "\n",
    "    Returns:\n",
    "        - list: Llista de bigrames oberts √∫nics presents al text.\n",
    "                Cada element correspon a un bigrama generat a partir de les paraules.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Normalitzaci√≥ i extracci√≥ de paraules\n",
    "    text_lower = text.lower()\n",
    "    words_in_text = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "    \n",
    "    # 2. Uni√≥ dels conjunts de bigrames de totes les paraules\n",
    "    text_bigrams_set = set()\n",
    "    for word in words_in_text:\n",
    "        # Cridem la funci√≥ feta anteriorment (n=2 per bigrames)\n",
    "        ngrams = get_open_ngrams(word, n=2, include_boundaries=True)\n",
    "        text_bigrams_set.update(ngrams)\n",
    "        \n",
    "    # 3. Retornem com a llista\n",
    "    return list(text_bigrams_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40f1eaa",
   "metadata": {
    "id": "c40f1eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de textos processats: 17600\n",
      "Exemple del primer text (primers 10 bigrames): ['‡∏∞‡∏ü', '‡∏ô‡∏ä', '‡∏°‡πÑ', '‡∏™‡∏ü', '‡∏ß‡πÑ', '‡πÅ‡∏™', '‡∏ì‡∏™', '‡∏•‡∏ß', '‡∏´‡∏ß', '‡∏∞‡∏™']\n"
     ]
    }
   ],
   "source": [
    "# Extreu els n-grames oberts dels textos del conjunt de training a la variable X_train_ngrams[:][:]\n",
    "\n",
    "X_train_ngrams = []\n",
    "\n",
    "for text in X_train:\n",
    "    bigrams_list = extract_text_open_bigrams(text)\n",
    "    X_train_ngrams.append(bigrams_list)\n",
    "\n",
    "print(f\"Nombre de textos processats: {len(X_train_ngrams)}\")\n",
    "if len(X_train_ngrams) > 0:\n",
    "    print(f\"Exemple del primer text (primers 10 bigrames): {X_train_ngrams[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b034d",
   "metadata": {
    "id": "748b034d"
   },
   "source": [
    "Extreu els n-grames oberts dels textos del conjunt de test (`X_test_ngrams`[:][:]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "028be985",
   "metadata": {
    "id": "028be985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processats 4400 textos del conjunt de test.\n",
      "Exemple (primer text del test): ['Êõ¥„ÅÇ', '„ÉüÂ°´', '„ÇãÂõû', '„Åç„Åü', 'Èï∑„Åï', 'Âä©„Çã', '„Ç¢„Åï', '„Å´‰∏≠', 'Áî®„Çä', 'ÁÑ°ÊΩú']\n"
     ]
    }
   ],
   "source": [
    "# Extreu els n-grames oberts dels textos del conjunt de test a la variable X_test_ngrams[:][:]\n",
    "\n",
    "X_test_ngrams = []\n",
    "\n",
    "for text in X_test:\n",
    "    bigrams_list = extract_text_open_bigrams(text)\n",
    "    X_test_ngrams.append(bigrams_list)\n",
    "\n",
    "print(f\"Processats {len(X_test_ngrams)} textos del conjunt de test.\")\n",
    "print(f\"Exemple (primer text del test): {X_test_ngrams[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11125ac",
   "metadata": {
    "id": "a11125ac"
   },
   "source": [
    "El seg√ºent objectiu √©s aplicar un **classificador ingenu de Bayes** per detectar les lleg√ºes dels textos del conjunt de test.\n",
    "\n",
    "Per fer-ho hem de convertir els $n$-grames oberts extrets en **representacions num√®riques** i ho farem amb el m√®tode TF-IDF, preparant-los per al classificador.\n",
    "\n",
    "> El **TF-IDF** (de Term Frequency ‚Äì Inverse Document Frequency) √©s una t√®cnica molt utilitzada en processament del llenguatge natural per representar textos de manera num√®rica i mesurar la import√†ncia de cada element del text (en el nostre cas bigrames) dins d‚Äôun conjunt de documents.\n",
    "\n",
    "TF-IDF combina dues idees simples:\n",
    "\n",
    "+ TF ‚Äî Term Frequency (freq√º√®ncia del terme):  Mesura quantes vegades apareix un element o terme dins d‚Äôun document.\n",
    "\n",
    "$$TF(t, d) = \\frac{\\text{nombre de vegades que el terme } t \\text{ apareix a } d}{\\text{nombre total de paraules al document } d}$$\n",
    "\n",
    "+ IDF ‚Äî Inverse Document Frequency (freq√º√®ncia inversa del document): Mesura com d‚Äôespecial √©s un element dins del conjunt total de documents.\n",
    "\n",
    "$$ IDF(t) = \\log\\left(\\frac{N}{1 + n_t}\\right) $$\n",
    "\n",
    "On:\n",
    "+ $N$ = nombre total de documents\n",
    "+ $n_t$ = nombre de documents on apareix l'element $t$\n",
    "\n",
    "üëâ Si una paraula (o bigrama) apareix a gaireb√© tots els documents (com `el`, `una`, `de`), el seu IDF √©s baix.\n",
    "\n",
    "üëâ Si nom√©s apareix en pocs, el seu IDF √©s alt ‚Äî i, per tant, √©s m√©s discriminativa.\n",
    "\n",
    "El **TF-IDF** √©s:\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
    "\n",
    "Aix√≠, el elements:\n",
    "+ freq√ºents dins d‚Äôun text (alt TF)\n",
    "+ per√≤ poc freq√ºents en la resta del corpus (alt IDF)\n",
    "\n",
    "reben m√©s pes en la representaci√≥ num√®rica.\n",
    "\n",
    "Suposa dos textos:\n",
    "1. ‚ÄúEl gat dorm al sof√†.‚Äù\n",
    "2. ‚ÄúEl gos juga al parc.‚Äù\n",
    "\n",
    "Les paraules `el` i `al` apareixen a tots dos ‚Üí TF alt per√≤ IDF baix.\n",
    "Les paraules `gat`, `gos`, `sof√†` o `parc` apareixen nom√©s a un ‚Üí TF moderat per√≤ IDF alt ‚Üí m√©s importants per diferenciar els textos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaadf0d1",
   "metadata": {
    "id": "aaadf0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n",
      "Shape of X_train_tfidf: (17600, 807585)\n",
      "Shape of X_test_tfidf: (4400, 807585) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF ja est√† incl√≤s a `scikit-learn`, una de les llibreries m√©s\n",
    "# populars per a machine learning i el podem fer servir aix√≠ en el\n",
    "# nostre cas:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "vectorizer.fit(X_train_ngrams)\n",
    "\n",
    "X_train_tfidf = vectorizer.transform(X_train_ngrams)\n",
    "X_test_tfidf = vectorizer.transform(X_test_ngrams)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AVVFL4AUCIEf",
   "metadata": {
    "id": "AVVFL4AUCIEf"
   },
   "source": [
    "Les matrius que ha generat la cel¬∑la anterior estan guardats amb una estructura de python que es diu \"Compressed Sparse Row sparse matrix\".\n",
    "\n",
    "Pensa que tens una taula enorme de nombres, per√≤ la majoria s√≥n zeros. Guardar tots aquests zeros √©s una p√®rdua de mem√≤ria i de temps.\n",
    "\n",
    "Per aix√≤ en Python (amb scipy) sovint es fan servir matrius disperses (sparse matrices), i una de les m√©s habituals √©s el format Compressed Sparse Row (CSR).\n",
    "\n",
    "En lloc de guardar tots els elements de la matriu, una CSR nom√©s guarda:\n",
    "+ Els valors no zero\n",
    "+ La columna de cada valor no zero\n",
    "+ On comen√ßa i acaba cada fila dins d‚Äôaquestes llistes\n",
    "\n",
    "Per√≤ tu, com a usuari, no cal que et preocupis gaire de com ho fa per dins:\n",
    "la tractes gaireb√© com si fos una matriu de NumPy, per√≤ amb algunes difer√®ncies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160aa057",
   "metadata": {
    "id": "160aa057"
   },
   "source": [
    "### Tasca 4: Implementaci√≥ d'un classificador ingenu de Bayes\n",
    "\n",
    "Has d'entrenar un classificador Naive Bayes amb les caracter√≠stiques TF-IDF del conjunt d'entrenament i les seves etiquetes de llengua corresponents.\n",
    "\n",
    "El model calcula la probabilitat que un text $d$ pertanyi a una llengua $c$:\n",
    "\n",
    "$$P(c \\mid d) \\propto P(c) \\prod_{i=1}^{V} P(w_i \\mid c)^{\\, f_i}$$\n",
    "\n",
    "On:\n",
    "+ $P(c)$: probabilitat pr√®via de la classe (per exemple, % de textos d'una llengua; en el nostre cas totes les lleng√ºes tenen la mateixa probabilitat).\n",
    "+ $w_i$: bigrama i-√®ssim del vocabulari.\n",
    "+ $f_i$: nombre de vegades que el bigrama $w_i$ apareix al text $d$.\n",
    "+ $P(w_i \\mid c)$: probabilitat que el bigrama $w_i$ aparegui en textos de la classe $c$.\n",
    "+ $V$: mida del vocabulari.\n",
    "\n",
    "El classificador escull la classe amb probabilitat m√©s alta.\n",
    "\n",
    "#### Com es calcula $P(w_i \\mid c)$?\n",
    "\n",
    "Normalment es calcula amb un model multinomial amb Laplace *smoothing* (per evitar zeros):\n",
    "\n",
    "$$P(w_i \\mid c) =\n",
    "\\frac{N_{i,c} + 1}{\\sum_{j=1}^{V} N_{j,c} + V}$$\n",
    "\n",
    "On:\n",
    "+ $N_{i,c}$: nombre total de vegades que la paraula $w_i$ apareix en tots els documents de la classe c.\n",
    "+ $V$: nombre total de paraules diferents del vocabulari.\n",
    "\n",
    "Aquesta f√≥rmula ve directament de la distribuci√≥ multinomial, perqu√® compta freq√º√®ncies de paraules dins d‚Äôuna ‚Äúbossa‚Äù pr√≤pia de cada classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmvW91jfvi40",
   "metadata": {
    "id": "mmvW91jfvi40"
   },
   "source": [
    "Fes una implementaci√≥ teva del model multinomial, √©s a dir, de les funcions que creen el model i apliquen el model al test:\n",
    "\n",
    " `MultinomialNBfit(X_train_tfidf, y_train))`\n",
    "\n",
    " i\n",
    "\n",
    " `MultinomialNBpredict(model, X_test_tfidf, y_test))`\n",
    "\n",
    "Imprimeix quina `accuracy` obtens. Si tot funciona correctament, hauries d'aconseguir una *accuracy* per sobre el 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "031bf273-21e6-44e5-809f-492c4467e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "class MultinomialNB:\n",
    "    \"\"\"\n",
    "    Implementaci√≥ del classificador Ingenu de Bayes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Inicialitza el model.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): Par√†metre de suavitzat de Laplace.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.languages = None\n",
    "        self.Pprior = None\n",
    "        self.Pconditional = None\n",
    "        # Per tal de fer prediccions consecutives\n",
    "        # sense haver d'entrenar cada cop\n",
    "        # Necessitem guardar el vector \n",
    "        self.vectorizer = None\n",
    "        return\n",
    "\n",
    "    \"\"\"\n",
    "    M√àTODES AUXILIARS\n",
    "    \"\"\"\n",
    "    def get_open_ngrams(self, word: str, n: int, include_boundaries: bool = True) -> set:\n",
    "        \"\"\"\n",
    "        Genera n-grames oberts per a una paraula.\n",
    "\n",
    "        Args:\n",
    "            word (str): Paraula d'entrada.\n",
    "            n (int): Longitud de l'n-grama.\n",
    "            include_boundaries (bool): Indica si s'inclouen s√≠mbols de l√≠mit.\n",
    "\n",
    "        Returns:\n",
    "            set: Conjunt d'n-grames oberts.\n",
    "        \"\"\"\n",
    "        open_ngrams = set()\n",
    "        \n",
    "        # 1. Generar d'n-grames interns\n",
    "        if len(word) >= n:\n",
    "            for ngram_tuple in combinations(word, n):\n",
    "                open_ngrams.add(\"\".join(ngram_tuple))\n",
    "    \n",
    "        # 2. Boundaries de principi i final de paraula\n",
    "        if include_boundaries:\n",
    "            if n == 1:\n",
    "                open_ngrams.add(\"_\")\n",
    "            else:\n",
    "                # Inici\n",
    "                if len(word) >= 1 and len(word[1:]) >= (n - 2):\n",
    "                    for sub in combinations(word[1:], n - 2):\n",
    "                        open_ngrams.add(\"_\" + word[0] + \"\".join(sub))\n",
    "                # Final\n",
    "                if len(word) >= 1 and len(word[:-1]) >= (n - 2):\n",
    "                    for sub in combinations(word[:-1], n - 2):\n",
    "                        open_ngrams.add(\"\".join(sub) + word[-1] + \"_\")\n",
    "        return open_ngrams\n",
    "\n",
    "\n",
    "    def get_all_ngrams_from_text(self, text_list, n=2, include_boundaries=True):\n",
    "        \"\"\"\n",
    "        Converteix una llista de textos en una llista de llistes d'n-grames oberts.\n",
    "\n",
    "        Args:\n",
    "            text_list (list): Llista de textos d'entrada.\n",
    "            n (int): Longitud de l'n-grama.\n",
    "            include_boundaries (bool): Si s'han d'incloure marcadors de l√≠mit.\n",
    "\n",
    "        Returns:\n",
    "            list: Llista de llistes d'n-grames per text.\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_texts = []\n",
    "        \n",
    "        for text in text_list:\n",
    "            # Normalitzaci√≥ b√†sica\n",
    "            if not isinstance(text, str): \n",
    "                processed_texts.append([])\n",
    "                continue\n",
    "                \n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            ngrams_set = set()\n",
    "            for word in words:\n",
    "                ngrams_set.update(self.get_open_ngrams(word, n, include_boundaries))\n",
    "            \n",
    "            processed_texts.append(list(ngrams_set))\n",
    "        return processed_texts\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    PIPELINE: nom√©s cal proporcionar dades i el m√®tode s'encarrega de tot el proc√©s\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def get_text_label_from_df(self, path='dataset.csv'):\n",
    "        \n",
    "        dataset_file_path = path\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(dataset_file_path)\n",
    "        \n",
    "            # Extracci√≥ de textes i etiquetes\n",
    "            texts = df['Text'].tolist()\n",
    "            labels = df['language'].tolist()\n",
    "\n",
    "            return texts, labels\n",
    "\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Fitxer '{dataset_file_path}' no trobat.\")\n",
    "            print(\"Assegura't que dataset.csv est√† accessible.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al carregar el fitxer: {e}\")\n",
    "            \n",
    "    def do_model(self, path='dataset.csv', n=2):\n",
    "        texts, labels = self.get_text_label_from_df(path)\n",
    "\n",
    "        X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "            texts, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        X_train_ngrams = self.get_all_ngrams_from_text(X_train_raw, n=n, include_boundaries=True)\n",
    "        X_test_ngrams = self.get_all_ngrams_from_text(X_test_raw, n=n, include_boundaries=True)\n",
    "\n",
    "        if self.vectorizer is None:\n",
    "                self.vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        X_train_tfidf = self.vectorizer.fit_transform(X_train_ngrams)\n",
    "        X_test_tfidf = self.vectorizer.transform(X_test_ngrams)\n",
    "\n",
    "        # entrenar AQUEST objecte, no un nou model\n",
    "        self.MultinomialNBfit(X_train_tfidf, y_train)\n",
    "\n",
    "        # crida opcional, nom√©s per provar\n",
    "        y_pred = self.MultinomialNBpredict(X_test_tfidf, y_test)\n",
    "\n",
    "        # retornes self (model entrenat) \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict_language(self, text, path='dataset.csv', n=2):\n",
    "        \"\"\"\n",
    "        Prediu l'idioma d'un √∫nic text. Entrena el model amb dataset.csv\n",
    "        la primera vegada que es crida, i despr√©s fa la predicci√≥ per al text rebut.\n",
    "        Guarda el vectorizer com a atribut per reutilitzar-lo.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Si el model (probabilitats condicionals) o el vectorizer encara no estan entrenats, entrenar-los ara\n",
    "        if self.Pconditional is None or self.vectorizer is None:\n",
    "            self.do_model(path=path, n=n)\n",
    "    \n",
    "        # 2. Generar n-grames per al text d'entrada\n",
    "        ngrams = self.get_all_ngrams_from_text([text], n=n, include_boundaries=True)\n",
    "    \n",
    "        # 3. Vectoritzar el text amb el vectorizer ja entrenat\n",
    "        X = self.vectorizer.transform(ngrams)\n",
    "    \n",
    "        # 4. Fer la predicci√≥ usant el model entrenat\n",
    "        pred = self.MultinomialNBpredict(X, y=None)[0]\n",
    "        return pred\n",
    "        \n",
    "    \"\"\"\n",
    "    M√àTODES DE LA PR√ÄCTICA\n",
    "    \"\"\"\n",
    "\n",
    "    def MultinomialNBfit(self, X, y):\n",
    "            \"\"\"\n",
    "            Entrena el model Multinomial Naive Bayes amb suavitzat de Laplace.\n",
    "\n",
    "            Args:\n",
    "                X (sparse matrix): Matriu de caracter√≠stiques d'entrenament.\n",
    "                y (array-like): Etiquetes de classe.\n",
    "\n",
    "            Returns:\n",
    "                self: Retorna la inst√†ncia entrenada.\n",
    "            \"\"\"\n",
    "            y = np.array(y)\n",
    "\n",
    "            # 1. Inicialitzaci√≥ de classes si encara no estan definides\n",
    "            if self.languages == None:\n",
    "                self.languages = np.unique(y)\n",
    "            num_lang = len(self.languages)\n",
    "            y = np.array(y)\n",
    "\n",
    "            # 2. Nombre total d'n-grames (dimensions de les feature)\n",
    "            num_ngram = X.shape[1]\n",
    "            denominador = 0\n",
    "\n",
    "            if self.Pprior == None:\n",
    "                self.Pprior = np.zeros(num_lang, dtype=np.float32)\n",
    "            if self.Pconditional == None:    \n",
    "                self.Pconditional = np.zeros((num_lang, num_ngram), dtype=np.float32)\n",
    "\n",
    "            # 3. C√†lcul de probabilitats per a cada classe\n",
    "            for index, language in enumerate(self.languages):\n",
    "                actual_lang_mask = (y==language)\n",
    "\n",
    "                n_docs_actual_lang = np.sum(actual_lang_mask)\n",
    "                self.Pprior[index] = np.log(n_docs_actual_lang / len(y))\n",
    "\n",
    "                n_ngram_actual_lang = np.array(X[actual_lang_mask, :].sum(axis=0)).flatten()\n",
    "\n",
    "                total_ngram_lang = n_ngram_actual_lang.sum()\n",
    "            \n",
    "                numerador = np.log(n_ngram_actual_lang + self.alpha)\n",
    "                denominador = np.log(total_ngram_lang + num_ngram*self.alpha)\n",
    "\n",
    "                self.Pconditional[index, :] = numerador - denominador\n",
    "\n",
    "            return self\n",
    "    \n",
    "    def MultinomialNBpredict(self, X, y):\n",
    "            \"\"\"\n",
    "            Genera prediccions per a un conjunt de dades utilitzant el model entrenat.\n",
    "\n",
    "            Calcula l'score logar√≠tmic per a cada classe i selecciona la classe\n",
    "            amb major probabilitat.\n",
    "\n",
    "            Args:\n",
    "                X (sparse matrix): Matriu de caracter√≠stiques de test.\n",
    "                y (array-like): Etiquetes reals (no s'utilitzen en el c√†lcul).\n",
    "\n",
    "            Returns:\n",
    "                np.ndarray: Array amb les classes predites.\n",
    "            \"\"\"\n",
    "\n",
    "            # 1. Producte escalar entre documents i matriu de probabilitats condicionals\n",
    "            accuracy_score = X @ self.Pconditional.T    \n",
    "            accuracy_score += self.Pprior\n",
    "\n",
    "            # 2. Selecci√≥ de la classe amb score m√†xim\n",
    "            final_scores = np.argmax(accuracy_score, axis=1)\n",
    "            final_score = self.languages[final_scores]\n",
    "        \n",
    "            return final_score        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fca5a50d-c3bc-464c-8ab6-a31124b6c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.45%\n"
     ]
    }
   ],
   "source": [
    "# 1. Instanciar i Entrenar\n",
    "model = MultinomialNB()\n",
    "model.MultinomialNBfit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Predir\n",
    "y_pred = model.MultinomialNBpredict(X_test_tfidf, y_test)\n",
    "\n",
    "# 3. Calcular Accuracy (Manualment o amb sklearn)\n",
    "correctes = (y_pred == y_test) # Array de True/False\n",
    "accuracy = np.mean(correctes)  # La mitjana de True √©s el % d'encerts\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f3e20b-b86b-4200-a31f-4f178ae0e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Indonesian\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=1.0)\n",
    "\n",
    "# Primera vegada: es comprovar√† que no est√† entrenat i usar√† \"dataset.csv\"\n",
    "lang = nb.predict_language(\"This is a test sentence.\")\n",
    "print(lang)\n",
    "\n",
    "# Seg√ºents crides: ja est√† entrenat, nom√©s vectoritza i prediu\n",
    "lang2 = nb.predict_language(\"Saya suka kacang-kacangan dengan chorizo dan saus ikan kod\")\n",
    "print(lang2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f998f9-9dd0-402c-aebd-62d666bac7b0",
   "metadata": {},
   "source": [
    "# Qu√® possar al .py:\n",
    "\n",
    "- Classe Mulinomial\n",
    "\n",
    "- Fer el info de les binomials:\n",
    "\n",
    "\n",
    "        - Fer m√®todes per extreure info:   5 bigrames que mes surtin, paraules coincidents(?), llista amb percentatges que surt cada bigrama\n",
    "\n",
    "- Preguntar al chat com es pot modificar aix√≤\n",
    "\n",
    "- Mirar examen anterior per afegir-hi m√®todes\n",
    "\n",
    "- Mirar qu√® passa si dropeas idioma\n",
    "\n",
    "- Preguntes te√≤riques: Qu√® vol dir que l'angles tingui molt error? (molt simple i al ser llatina t√© moltes coses semblants)\n",
    "\n",
    "- Fer m√®tode de give accuracy, amb diferents m√®todes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

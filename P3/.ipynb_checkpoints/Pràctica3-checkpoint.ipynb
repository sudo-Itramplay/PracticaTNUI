{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kCuUg8CeVCjU",
   "metadata": {
    "id": "kCuUg8CeVCjU"
   },
   "source": [
    "# **Pr√†ctica 3: n-Grames Oberts i Classificador Ingenu de Bayes**\n",
    "\n",
    "Aquest exercici explora els **n-Grames Oberts** com a **representaci√≥ de paraules**, demostrant la seva robustesa davant del soroll i del desordre.\n",
    "\n",
    "Definirem un **$n$-grama** com una configuraci√≥ espec√≠fica de $n$ lletres consecutives dins d'una paraula.\n",
    "\n",
    "+ Per exemple, `at` i `i√≥` s√≥n alguns dels bigrames ($2$-grames) que formen part de la paraula `atenci√≥`. Fent servir aquest concepte, podriem dir que la paraula `atenci√≥` es pot representar pel conjunt de bigrames `{at, te, en, nc, ci, i√≥}`.\n",
    "\n",
    "Un **$n$-grama obert** √©s una configuraci√≥ que defineix un subconjunt ordenat ‚Äîper√≤ no necess√†riament contigu‚Äî de $n$ lletres dins d'una paraula.\n",
    "\n",
    "+ Per exemple, si considerem la paraula `hello`, el conjunt de bigrames oberts √©s `{he, hl, ho, el, eo, ll, lo}`.\n",
    "+ En moltes ocasions tamb√© √©s √∫til considerar el comen√ßament i el final d‚Äôuna paraula com a casos especials, tenint en compte l‚Äôespai en blanc.\n",
    "En aquest cas, per exemple, la paraula `hello` genera el seg√ºent conjunt ampliat de bigrames oberts:\n",
    "`{ _h, he, hl, ho, el, eo, ll, lo, o_ }`.\n",
    "\n",
    "> El cervell hum√† pot llegir frases amb les lletres internes desordenades sempre que: La primera i l‚Äô√∫ltima lletra de cada paraula es mantinguin al lloc, la paraula tingui una longitud suficient, el context de la frase sigui clar. Aix√≤ passa perqu√®, quan llegim, no analitzem cada lletra una per una, sin√≥ que reconeixem les paraules com a formes globals i utilitzem el context sem√†ntic per omplir els buits. El cervell fa una mena de ‚Äúcorrecci√≥ autom√†tica‚Äù basant-se en les paraules que espera veure.\n",
    "\n",
    "> Exemple: `Segns un etsdui de la Uinveristtat de Cmarbigde, el crevell pot lgegir paaulebs ambl les lleterres barajades i mab srrol sense mases dfiicultats.`\n",
    "\n",
    "> Els n-grames oberts (open n-grams) donen una explicaci√≥ molt natural de per qu√® podem llegir/reconeixer paraules amb les lletres internes desordenades.\n",
    "\n",
    "La definici√≥ dels $n$-grames oberts es basa en dos aspectes clau:\n",
    "+ Ordenaci√≥: Les lletres han d‚Äôapar√®ixer en el mateix ordre que en la paraula original (per exemple, `NBK` √©s un 3-grama obert v√†lid de la paraula `NOTEBOOK`, per√≤ `KBN` no ho √©s, perqu√® la $\\text{K}$ apareix despr√©s de la $\\text{B}$ i la $\\text{N}$ a la paraula).\n",
    "+ No contig√ºitat: Les lletres poden estar separades per qualsevol nombre d‚Äôaltres lletres (aix√≤ √©s el que fa que la representaci√≥ sigui robusta davant de soroll, com ara errors tipogr√†fics o lletres que falten).\n",
    "\n",
    "El nombre total de $n$-grames oberts per a una paraula de $L$ lletres ve donat pel coeficient binomial $\\binom{L}{n}$.\n",
    "\n",
    "Per a `NOTEBOOK` ($L=8$ i $n=3$), el total √©s:\n",
    "\n",
    "$$\\binom{8}{3} = \\frac{8!}{3!(8-3)!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = \\mathbf{56}$$."
   ]
  },
  {
   "cell_type": "raw",
   "id": "98059fa6-f24e-42dc-88d2-c6586e6cdd26",
   "metadata": {},
   "source": [
    "n-grama ordenat vol dri que, les lletres que hem passat no les farem servir, fem servir _ per representar un espai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJ03cE5ibYSI",
   "metadata": {
    "id": "UJ03cE5ibYSI"
   },
   "source": [
    "## Part 1: Extracci√≥ de Caracter√≠stiques ‚Äî $N$-grames oberts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iems6AUItrER",
   "metadata": {
    "id": "iems6AUItrER"
   },
   "source": [
    "### Tasca 1\n",
    "\n",
    "Implementa una per extreure els $n$-grames oberts d‚Äôuna paraula, que ens servir√† com un nou tipus de representaci√≥ de les paraules d'un text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "87042836-1c8b-4d03-8632-3470033143ce",
   "metadata": {},
   "source": [
    "#itertools combinations\n",
    "#iterable, r # r √©s len de cada combinaci√≥, (si passem 2 fara combinacions de len 29\n",
    "\n",
    "# OJO amb les tuples\n",
    "\n",
    "# Els iteradors no s√≥n imprimibles si fem un combinations, no podrem imprimirho, no com a conjunt\n",
    "\n",
    "\"\".join(word[index]\n",
    "\n",
    "\n",
    "# ngrams tindr√† fors, no passa res\n",
    "\n",
    "        # 1r fer n grames\n",
    "        # 2n boundaries\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b32410-a2ce-467f-a27d-5b6bb82b133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_open_ngrams(word: str, n: int, include_boundaries: bool = True) -> set:\n",
    "    \"\"\"Genera un conjunt d‚Äôn-grams oberts per a una paraula donada, amb un tractament espec√≠fic dels l√≠mits.\n",
    "\n",
    "    Args:\n",
    "        word (str): La paraula d‚Äôentrada.\n",
    "        n (int): L‚Äôordre de l‚Äôn-gram (per exemple, 2 per a bigrames, 3 per a trigrames).\n",
    "        include_boundaries (bool): Si √©s True, afegeix un gui√≥ baix '_' al principi i al\n",
    "                                final de la paraula per incloure els l√≠mits inicial/final.\n",
    "\n",
    "    Returns:\n",
    "        set: Un conjunt d‚Äôn-grams oberts √∫nics.\n",
    "    \"\"\"\n",
    "    \n",
    "    open_ngrams = set()\n",
    "\n",
    "    # 1. Generar N-grams sense boundaries (Core)\n",
    "    # combinations retorna tuples, fem servir join per convertir-les a string\n",
    "    if len(word) >= n:\n",
    "        for ngram_tuple in combinations(word, n):\n",
    "            open_ngrams.add(\"\".join(ngram_tuple))\n",
    "\n",
    "    # 2. Generar boundaries (si es demana)\n",
    "    if include_boundaries:\n",
    "        \n",
    "        # Cas especial: n == 1\n",
    "        # Simplement afegim el marcador de l√≠mit \"_\"\n",
    "        if n == 1:\n",
    "            open_ngrams.add(\"_\")\n",
    "        \n",
    "        # Casos generals (n >= 2)\n",
    "        else:\n",
    "            # Regla: '_' + primer car√†cter + (n-2 car√†cters de la resta)\n",
    "            # Aix√≤ assegura que el l√≠mit nom√©s toca el primer car√†cter real.\n",
    "            first_char = word[0]\n",
    "            rest_of_word = word[1:]\n",
    "            \n",
    "            # Necessitem triar (n-2) car√†cters de la resta de la paraula\n",
    "            # Si n=2, triem 0 car√†cters (el bucle s'executa una vegada amb string buit)\n",
    "            if len(rest_of_word) >= (n - 2):\n",
    "                for sub_combo in combinations(rest_of_word, n - 2):\n",
    "                    ngram = \"_\" + first_char + \"\".join(sub_combo)\n",
    "                    open_ngrams.add(ngram)\n",
    "\n",
    "            # Regla: (n-2 car√†cters de l'inici fins al pen√∫ltim) + √∫ltim car√†cter + '_'\n",
    "            last_char = word[-1]\n",
    "            start_of_word = word[:-1]\n",
    "            \n",
    "            if len(start_of_word) >= (n - 2):\n",
    "                for sub_combo in combinations(start_of_word, n - 2):\n",
    "                    ngram = \"\".join(sub_combo) + last_char + \"_\"\n",
    "                    open_ngrams.add(ngram)\n",
    "\n",
    "    return open_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440894c1",
   "metadata": {
    "id": "440894c1"
   },
   "source": [
    "Executa el test de la funci√≥ `get_open_ngrams` amb la paraula `hello` i comprova que funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e11350",
   "metadata": {
    "id": "71e11350"
   },
   "outputs": [],
   "source": [
    "word_to_test = \"hello\"\n",
    "n_gram_order_test = 2\n",
    "\n",
    "ngrams_with_boundaries = get_open_ngrams(word_to_test, n_gram_order_test, include_boundaries=True)\n",
    "\n",
    "print(ngrams_with_boundaries)\n",
    "\n",
    "assert ngrams_with_boundaries == {'_h', 'el', 'eo', 'he', 'hl', 'ho', 'll', 'lo', 'o_'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lrk-wL3iVCjW",
   "metadata": {
    "id": "lrk-wL3iVCjW"
   },
   "source": [
    "## Part 2: Col¬∑lisions entre paraules\n",
    "\n",
    "Podem avaluar la capacitat per representar paraules comprovant la **taxa de col¬∑lisi√≥ de paraules**, que es produeix quan dues paraules diferents tenen el mateix conjunt de caracter√≠stiques.\n",
    "\n",
    "Per fer-ho farem servir el fitxer `dataset.csv`, que cont√© frases en diferents idiomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "QUTCJQ9zklgm",
   "metadata": {
    "id": "QUTCJQ9zklgm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitxer carregat.\n",
      "Nombre de frases: 22000\n",
      "Exemple de frase: klement gottwaldi surnukeha palsameeriti ning paigutati mausoleumi surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundem√§rke  aastal viidi ta surnukeha mausoleumist √§ra ja kremeeriti zl√≠ni linn kandis aastatel ‚Äì nime gottwaldov ukrainas harkivi oblastis kandis zmiivi linn aastatel ‚Äì nime gotvald\n",
      "Exemple d'etiqueta: Estonian\n",
      "\n",
      "Distribuci√≥ de Llengues:\n",
      "Counter({'Estonian': 1000, 'Swedish': 1000, 'Thai': 1000, 'Tamil': 1000, 'Dutch': 1000, 'Japanese': 1000, 'Turkish': 1000, 'Latin': 1000, 'Urdu': 1000, 'Indonesian': 1000, 'Portugese': 1000, 'French': 1000, 'Chinese': 1000, 'Korean': 1000, 'Hindi': 1000, 'Spanish': 1000, 'Pushto': 1000, 'Persian': 1000, 'Romanian': 1000, 'Russian': 1000, 'English': 1000, 'Arabic': 1000})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "dataset_file_path = 'dataset.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(dataset_file_path)\n",
    "\n",
    "    # Extracci√≥ de textes i etiquetes\n",
    "    texts = df['Text'].tolist()\n",
    "    labels = df['language'].tolist()\n",
    "\n",
    "    print(\"Fitxer carregat.\")\n",
    "    print(f\"Nombre de frases: {len(texts)}\")\n",
    "    print(f\"Exemple de frase: {texts[0]}\")\n",
    "    print(f\"Exemple d'etiqueta: {labels[0]}\")\n",
    "    print(\"\\nDistribuci√≥ de Llengues:\")\n",
    "    print(Counter(labels))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Fitxer '{dataset_file_path}' no trobat.\")\n",
    "    print(\"Assegura't que dataset.csv est√† accessible.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al carregar el fitxer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abf1f52a-6eb9-40ab-9513-96b13110602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMb el dataset trobarem un index perr dir quina llengua √©s, en el segon sheet creo, i una frase en aquell idioma\n",
    "# Fer primers situacions de n grames de \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tZWkJiSfrC3_",
   "metadata": {
    "id": "tZWkJiSfrC3_"
   },
   "source": [
    "### Tasca 2\n",
    "\n",
    "Escriu un codi en Python que:\n",
    "\n",
    "1. Extreu totes les paraules √∫niques de `texts` i posa-les a un conjunt que es dir√† `unique_words`\n",
    "2. Genera, per a cada paraula, els bigrames oberts ($n$=2) i conta quants conjunts de bigrames √∫nics hi ha al dataset.\n",
    "3. Detecta si hi ha col¬∑lisions al conjunt de dades, √©s a dir, casos en qu√® diferents paraules tenen exactament el mateix conjunt d‚Äô$n$-grames.\n",
    "4. Imprimeix:\n",
    "    + El total de paraules **√∫niques** extretes.\n",
    "    + El total de conjunts **√∫nics** d‚Äôn-grames generats.\n",
    "    + El nombre de paraules √∫niques implicades en col¬∑lisions.\n",
    "    + 5 exemples de parellles de paraules en col¬∑lisi√≥."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2aef403-4269-4f20-af21-9ae266db5f96",
   "metadata": {},
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "# HA DE TARDAR MENYS D UN MINUT\n",
    "Indicaci√≥: Per extreure totes les paraules d'un text pots fer servir aquest codi:\n",
    "\n",
    "text_lower = text.lower()\n",
    "words_in_text = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "#BUSCAR INTERPRET DE REGEX\n",
    "# 1. Extreu totes les paraules √∫niques de 'texts'\n",
    "\n",
    "\n",
    "\n",
    "# 2. Genera els bigrames oberts (n=2) amb l√≠mits per a cada paraula √∫nica\n",
    "\n",
    "\n",
    "\n",
    "# 3. Identifica i compta les col¬∑lisions\n",
    "\n",
    "\n",
    "# 4. Compta i imprimeix: el notal de paraules √∫niques extretes;\n",
    "# el nombre total de conjunts √∫nics d‚Äôn-grames generats;\n",
    "# el nombre total de paraules √∫niques implicades en col¬∑lisions;\n",
    "# 5 exemples de paraules que col¬∑lisionen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "new_collision_code",
   "metadata": {
    "id": "new_collision_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paraules √∫niques extretes: 278630\n",
      "Total conjunts √∫nics d'n-grames: 277959\n",
      "Nombre de paraules implicades en col¬∑lisions: 1335\n",
      "Percentatge de col¬∑lisi√≥: 0.48%\n",
      "\n",
      "5 exemples de grups de paraules en col¬∑lisi√≥:\n",
      "  1. ['ideii', 'idei'] -> Comparteixen els mateixos bigrames\n",
      "  2. ['creative', 'creatieve'] -> Comparteixen els mateixos bigrames\n",
      "  3. ['–∏–Ω–¥–∏', '–∏–Ω–¥–∏–∏'] -> Comparteixen els mateixos bigrames\n",
      "  4. ['santana', 'santanna'] -> Comparteixen els mateixos bigrames\n",
      "  5. ['chrisitiana', 'christiana'] -> Comparteixen els mateixos bigrames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Extreu totes les paraules √∫niques de 'texts'\n",
    "unique_words = set()\n",
    "\n",
    "for text in texts:\n",
    "    if isinstance(text, str): # Verificaci√≥ de seguretat per si hi ha NaNs\n",
    "        # Utilitzem regex per trobar paraules (\\w+), convertim a min√∫scules\n",
    "        # per evitar que \"Hola\" i \"hola\" siguin diferents.\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        unique_words.update(words)\n",
    "\n",
    "# 2. Genera els bigrames oberts (n=2) amb l√≠mits per a cada paraula √∫nica\n",
    "# Utilitzem un diccionari on:\n",
    "# Clau: Una representaci√≥ immutable del conjunt d'n-grams (frozenset o tuple ordenada)\n",
    "# Valor: Llista de paraules que generen aquest conjunt\n",
    "ngram_signature_map = defaultdict(list)\n",
    "\n",
    "for word in unique_words:\n",
    "    # Generem el set\n",
    "    ngrams_set = get_open_ngrams(word, n=2, include_boundaries=True)\n",
    "    \n",
    "    # Convertim el set a una tupla ordenada per poder usar-la com a clau de diccionari (hashable)\n",
    "    ngrams_signature = tuple(sorted(list(ngrams_set)))\n",
    "    \n",
    "    # Guardem la paraula sota aquesta signatura\n",
    "    ngram_signature_map[ngrams_signature].append(word)\n",
    "\n",
    "# 3. Identifica i compta les col¬∑lisions\n",
    "# Una col¬∑lisi√≥ existeix si la llista de paraules per a una clau t√© longitud > 1\n",
    "collision_groups = []\n",
    "words_in_collision_count = 0\n",
    "\n",
    "for signature, words_list in ngram_signature_map.items():\n",
    "    if len(words_list) > 1:\n",
    "        collision_groups.append(words_list)\n",
    "        words_in_collision_count += len(words_list)\n",
    "\n",
    "# 4. Compta i imprimeix resultats\n",
    "total_unique_words = len(unique_words)\n",
    "total_unique_ngram_sets = len(ngram_signature_map)\n",
    "\n",
    "\n",
    "print(f\"Total paraules √∫niques extretes: {total_unique_words}\")\n",
    "print(f\"Total conjunts √∫nics d'n-grames: {total_unique_ngram_sets}\")\n",
    "print(f\"Nombre de paraules implicades en col¬∑lisions: {words_in_collision_count}\")\n",
    "\n",
    "# C√†lcul del percentatge de col¬∑lisi√≥\n",
    "if total_unique_words > 0:\n",
    "    percentatge = (words_in_collision_count / total_unique_words) * 100\n",
    "    print(f\"Percentatge de col¬∑lisi√≥: {percentatge:.2f}%\")\n",
    "\n",
    "print(\"\\n5 exemples de grups de paraules en col¬∑lisi√≥:\")\n",
    "# Ordenem per longitud del grup per veure col¬∑lisions interessants, o agafem els primers 5\n",
    "# Aqu√≠ mostrem els primers 5 trobats:\n",
    "for i, group in enumerate(collision_groups[:5]):\n",
    "    print(f\"  {i+1}. {group} -> Comparteixen els mateixos bigrames\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a549a7-2d36-42c9-ac85-ef71958e47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCI√ì DEL CODI SUPERIOR\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analitza_collisions_ngrams(texts, n=2, include_boundaries=True, max_examples=5):\n",
    "    \"\"\"\n",
    "    Analitza col¬∑lisions de conjunts d'open n-grams per paraula.\n",
    "\n",
    "    Retorna un diccionari amb:\n",
    "      - total_unique_words\n",
    "      - total_unique_ngram_sets\n",
    "      - words_in_collision_count\n",
    "      - percentatge_col_lisio\n",
    "      - collision_groups (llista de llistes de paraules)\n",
    "    \"\"\"\n",
    "    # 1. Extreu totes les paraules √∫niques de 'texts'\n",
    "    unique_words = set()\n",
    "\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            unique_words.update(words)\n",
    "\n",
    "    # 2. Genera els n-grams oberts per a cada paraula √∫nica\n",
    "    ngram_signature_map = defaultdict(list)\n",
    "\n",
    "    for word in unique_words:\n",
    "        ngrams_set = get_open_ngrams(word, n=n, include_boundaries=include_boundaries)\n",
    "        ngrams_signature = tuple(sorted(list(ngrams_set)))\n",
    "        ngram_signature_map[ngrams_signature].append(word)\n",
    "\n",
    "    # 3. Identifica col¬∑lisions\n",
    "    collision_groups = []\n",
    "    words_in_collision_count = 0\n",
    "\n",
    "    for signature, words_list in ngram_signature_map.items():\n",
    "        if len(words_list) > 1:\n",
    "            collision_groups.append(words_list)\n",
    "            words_in_collision_count += len(words_list)\n",
    "\n",
    "    # 4. C√†lcul d‚Äôestad√≠stiques\n",
    "    total_unique_words = len(unique_words)\n",
    "    total_unique_ngram_sets = len(ngram_signature_map)\n",
    "\n",
    "    percentatge_col_lisio = 0.0\n",
    "    if total_unique_words > 0:\n",
    "        percentatge_col_lisio = (words_in_collision_count / total_unique_words) * 100\n",
    "\n",
    "    # imprimir alguns resultats\n",
    "    print(f\"Total paraules √∫niques extretes: {total_unique_words}\")\n",
    "    print(f\"Total conjunts √∫nics d'n-grames: {total_unique_ngram_sets}\")\n",
    "    print(f\"Nombre de paraules implicades en col¬∑lisions: {words_in_collision_count}\")\n",
    "    print(f\"Percentatge de col¬∑lisi√≥: {percentatge_col_lisio:.2f}%\")\n",
    "\n",
    "    print(f\"\\n{max_examples} exemples de grups de paraules en col¬∑lisi√≥:\")\n",
    "    for i, group in enumerate(collision_groups[:max_examples]):\n",
    "        print(f\"  {i+1}. {group} -> Comparteixen els mateixos n-grames\")\n",
    "\n",
    "    # Retornem les dades per si es vol reutilitzar\n",
    "    return {\n",
    "        \"total_unique_words\": total_unique_words,\n",
    "        \"total_unique_ngram_sets\": total_unique_ngram_sets,\n",
    "        \"words_in_collision_count\": words_in_collision_count,\n",
    "        \"percentatge_col_lisio\": percentatge_col_lisio,\n",
    "        \"collision_groups\": collision_groups,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe2fe8-7a44-4d57-933e-1c4c353ad145",
   "metadata": {},
   "source": [
    "Passos i pasos colisiona pq t√© igual length i igual ngrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcrk6JPFnn2E",
   "metadata": {
    "id": "rcrk6JPFnn2E"
   },
   "source": [
    "## Part 2: Classificador Ingenu de Bayes\n",
    "\n",
    "L'objectiu √©s fer un classificador que sigui capa√ß de predir la llengua d'una frase a partir de representar la frase com el conjunt de bigrames de les seves paraules.\n",
    "\n",
    "Per fer-ho comen√ßarem dividint el nostre dataset en una part de *training* i una de *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7afc4f79-1e05-460e-9922-18e95d3d29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empiesa la wea seria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f45edc7-56a2-4daf-ba95-2a79be8beb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts length: 17600\n",
      "Test texts length: 4400\n",
      "Training labels length: 17600\n",
      "Test labels length: 4400\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Les diferentcies entre x i y:\n",
    "\"\"\"\n",
    "texts √©s frase, label idioma, 0.2 per a test\n",
    "\n",
    "Parametre  shufle pot ser interessant per barrejar les dades\n",
    "\n",
    "X train √©s sense respostes, sense els idiomes el X test s√≥n les seves respostes (el 0.8)\n",
    "\n",
    "Y train √©s tamb√© sense respostes, per√≤ del 0.2\n",
    "\"\"\"\n",
    "\n",
    "# Aqu√≠ hauria d'anar df? NO, hem separat text i labels de df, per ara fer els sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training texts length: {len(X_train)}\")\n",
    "print(f\"Test texts length: {len(X_test)}\")\n",
    "print(f\"Training labels length: {len(y_train)}\")\n",
    "print(f\"Test labels length: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6452721",
   "metadata": {
    "id": "d6452721"
   },
   "source": [
    "### Tasca 3: Preparaci√≥ de les dades\n",
    "1. Extreu els bigrames oberts de cada un dels textos del conjunt d'entrenament (els bigrames oberts d'un text √©s el resultat de la uni√≥ dels conjunts de bigrames de totes les seves paraules).\n",
    "2. Un cop calculats, representa cada text d'entrenament amb una llista (`X_train_ngrams[:][:]`) de bigrames enlloc d'un conjunt. El primer √≠ndex √©s l'√≠ndex del text i el segon la llista dels seus bigrames.\n",
    "3. Mostra la llista que correspon al primer text d'entrenament.\n",
    "\n",
    "Indicaci√≥: Per extreure totes les paraules d'un text pots fer servir aquest codi:\n",
    "\n",
    "```python\n",
    "text_lower = text.lower()\n",
    "words_in_text = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "```\n",
    "\n",
    "X trains √©s llista de llistes\n",
    "\n",
    "\n",
    "Hem de construir en index 0 els bigrames oberts\n",
    "\n",
    "X_train[11] ha de sortir tonalli es un nombre personal...\n",
    "\n",
    "x_train_ngrams[], tindr√† TOTS els bigrames oberts unics de la frase 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2468d50-1d6e-4a4d-9d85-651da5e5c37e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c40f1eaa",
   "metadata": {
    "id": "c40f1eaa"
   },
   "outputs": [],
   "source": [
    "# Extreu els n-grames oberts dels textos del conjunt de training a la variable X_train_ngrams[:][:]\n",
    "import re\n",
    "\n",
    "# Xtrain √©s una llista de textos (str)\n",
    "# Retorna X_train_ngrams: llista de llistes de bigrames per cada text\n",
    "def get_all_ngrams_from_text(Xtrain = None, n=2: int, include_boundaries: bool = True):\n",
    "    X_train_ngrams = []  # llista de textos, cada element ser√† la llista de bigrames √∫nics d'aquell text\n",
    "    if Xtrain == None:\n",
    "        print(\"No data inserted\")\n",
    "        return []\n",
    "\n",
    "    for text in Xtrain:\n",
    "        text_lower = text.lower()\n",
    "        words_in_text = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "\n",
    "        # conjunt per fer la uni√≥ de bigrames de totes les paraules del text\n",
    "        ngrams_set = set()\n",
    "\n",
    "        for word in words_in_text:\n",
    "            word_ngrams = get_open_ngrams(word, n, include_boundaries)\n",
    "            ngrams_set.update(word_ngrams)\n",
    "\n",
    "        # aquest text queda representat com la llista dels seus bigrames √∫nics\n",
    "        X_train_ngrams.append(list(ngrams_set))\n",
    "\n",
    "    return X_train_ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b034d",
   "metadata": {
    "id": "748b034d"
   },
   "source": [
    "Extreu els n-grames oberts dels textos del conjunt de test (`X_test_ngrams`[:][:]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "028be985",
   "metadata": {
    "id": "028be985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(X_train[11])\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "X_train_ngrams = get_all_ngrams_from_text( X_train, n=2, include_boundaries=True)\n",
    "#print(X_train_ngrams[11])\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71425934-8a73-4f56-a971-e029e73a8e2e",
   "metadata": {},
   "source": [
    "Contar quant importants s√≥n per a cada idioma cada bigrama per posar-hi prob\n",
    "\n",
    "tf alt per√≤ id baix al i el apareix al dues frases un cop.\n",
    "\n",
    "sklearn t√© tf i idf vectorizer\n",
    "\n",
    "\n",
    "amb el lambda de tf indiquem que pasem raw content i no una folder o un path\n",
    "\n",
    "\n",
    "Les compressed sparse permet no haver de guardar tant zeros, sin√≥ guarda info dels elements que s√≥n 0\n",
    "\n",
    "\n",
    "Normalemnt les sparse es treballen amb scipy\n",
    "\n",
    "\n",
    "\n",
    "M√©s endavant (T4)\n",
    "\n",
    "c s√≥n idiomes, d √©s document\n",
    "\n",
    "\n",
    "En aquest data set totes tenen == prob, per√≤ cal calcular\n",
    "\n",
    "\n",
    "\n",
    "w √©s bigrama\n",
    "\n",
    "donar bigrama w_i donam prob de que sigui c\n",
    "\n",
    "Pensa en lbbar quantes vegades tingui, el meu idioma, aquest bigrama\n",
    "\n",
    "\n",
    "Quan hi ha moltes multiplicacions, fer √∫s de logaritmes per estalviar espai\n",
    "\n",
    "\n",
    "\n",
    "Quan diu paraula pensa que √©s un bigrama, error tipo\n",
    "\n",
    "Per comprovar els cosos scipy t√© la llibreria MultinomialNB creo\n",
    "\n",
    "Com scipy es open source podem agafar idees, per√≤ no podem copiar \n",
    "diu k no funcionara ns loko\n",
    "\n",
    "\n",
    "Detecta llengues en 95% accuracy aprox\n",
    "\n",
    "√âs m√©s facil fer Multinomial com a classe\n",
    "\n",
    "Fer una CustomMultinomialNB\n",
    "\n",
    "\n",
    "class i feature s√≥n prob cndicionada\n",
    "\n",
    "classes idiomes\n",
    "\n",
    "nfeatures bigrames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "M√®tode fit\n",
    "\n",
    "\n",
    "fit √©s per entrenar un model\n",
    "\n",
    "Hem d'aplicar formula bayes naive\n",
    "Per√≤ en aquest context si no parem de multiplicar hi haura tri 0, llavors agafarem logaritmes pq mul == suma, potencia == mul\n",
    "\n",
    "argmax √©s de totes les probabilitatss agafar la m√©s gran, que ser√† l'idioma que ell haur√† suposat del text\n",
    "\n",
    "FIT (XTRAIN Y TRAIN)\n",
    "\n",
    "TRAIN (XTEST Y TEST)\n",
    "\n",
    "funcions de multiNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Si no tenim en compte el tipus de dades tindrem overflow (allocate) Necessitarem float32 (perdem precissi√≥  a canvi de q funcioni)\n",
    "\n",
    "fit anir√† iiterant per a cada idioma\n",
    "\n",
    "\n",
    "EL +1 DE P(w_i | c) √©s un smoothing de Laplace per evitar que tots els 0 me tri jodan la mandanga, pots provar diferents per veure com canvia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pensa que el MultiNBfit ha de retornar self # si es treballa en classes no cal retornar res el retirn self √©s simplement per indicar que no retornem res m√©s que el model entrenat\n",
    "M√©s per clean code que altre wea\n",
    "\n",
    "\n",
    "\n",
    "Al final, el log, hem de mirar on guardar la probabilitat i √©s lliure\n",
    "\n",
    "Ell fa una llista xtest (llista de llistes [i,:]\n",
    "per tal que a index 0 tinc 1r idioma i aix√≠\n",
    "\n",
    "\n",
    "predict log ns que de argmax\n",
    "\n",
    "\n",
    "Ha de sortir 0.9545 el pred_custom -> Diu que si surt 90 o 80 tampoc passa res\n",
    "Per√≤ ns\n",
    "\n",
    "Treballar amb\n",
    "float32, treballar @, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11125ac",
   "metadata": {
    "id": "a11125ac"
   },
   "source": [
    "El seg√ºent objectiu √©s aplicar un **classificador ingenu de Bayes** per detectar les lleg√ºes dels textos del conjunt de test.\n",
    "\n",
    "Per fer-ho hem de convertir els $n$-grames oberts extrets en **representacions num√®riques** i ho farem amb el m√®tode TF-IDF, preparant-los per al classificador.\n",
    "\n",
    "> El **TF-IDF** (de Term Frequency ‚Äì Inverse Document Frequency) √©s una t√®cnica molt utilitzada en processament del llenguatge natural per representar textos de manera num√®rica i mesurar la import√†ncia de cada element del text (en el nostre cas bigrames) dins d‚Äôun conjunt de documents.\n",
    "\n",
    "TF-IDF combina dues idees simples:\n",
    "\n",
    "+ TF ‚Äî Term Frequency (freq√º√®ncia del terme):  Mesura quantes vegades apareix un element o terme dins d‚Äôun document.\n",
    "\n",
    "$$TF(t, d) = \\frac{\\text{nombre de vegades que el terme } t \\text{ apareix a } d}{\\text{nombre total de paraules al document } d}$$\n",
    "\n",
    "+ IDF ‚Äî Inverse Document Frequency (freq√º√®ncia inversa del document): Mesura com d‚Äôespecial √©s un element dins del conjunt total de documents.\n",
    "\n",
    "$$ IDF(t) = \\log\\left(\\frac{N}{1 + n_t}\\right) $$\n",
    "\n",
    "On:\n",
    "+ $N$ = nombre total de documents\n",
    "+ $n_t$ = nombre de documents on apareix l'element $t$\n",
    "\n",
    "üëâ Si una paraula (o bigrama) apareix a gaireb√© tots els documents (com `el`, `una`, `de`), el seu IDF √©s baix.\n",
    "\n",
    "üëâ Si nom√©s apareix en pocs, el seu IDF √©s alt ‚Äî i, per tant, √©s m√©s discriminativa.\n",
    "\n",
    "El **TF-IDF** √©s:\n",
    "\n",
    "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
    "\n",
    "Aix√≠, el elements:\n",
    "+ freq√ºents dins d‚Äôun text (alt TF)\n",
    "+ per√≤ poc freq√ºents en la resta del corpus (alt IDF)\n",
    "\n",
    "reben m√©s pes en la representaci√≥ num√®rica.\n",
    "\n",
    "Suposa dos textos:\n",
    "1. ‚ÄúEl gat dorm al sof√†.‚Äù\n",
    "2. ‚ÄúEl gos juga al parc.‚Äù\n",
    "\n",
    "Les paraules `el` i `al` apareixen a tots dos ‚Üí TF alt per√≤ IDF baix.\n",
    "Les paraules `gat`, `gos`, `sof√†` o `parc` apareixen nom√©s a un ‚Üí TF moderat per√≤ IDF alt ‚Üí m√©s importants per diferenciar els textos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21b08efc-8be3-4c14-98c6-4031310e70dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nY_train_ngrams=get_all_ngrams_from_text(Y_train, n=2, include_boundaries=True)\\nY_test_ngrams=get_all_ngrams_from_text(Y_test, n=2, include_boundaries=True)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extreu els n-grames oberts dels textos del conjunt de test a la variable X_test_ngrams[:][:]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Amb aix√≤ extret fem els n-grames \n",
    "X_train_ngrams=get_all_ngrams_from_text(X_train, n=2, include_boundaries=True)\n",
    "X_test_ngrams=get_all_ngrams_from_text(X_test, n=2, include_boundaries=True)\n",
    "\n",
    "\"\"\"\n",
    "Y_train_ngrams=get_all_ngrams_from_text(Y_train, n=2, include_boundaries=True)\n",
    "Y_test_ngrams=get_all_ngrams_from_text(Y_test, n=2, include_boundaries=True)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaadf0d1",
   "metadata": {
    "id": "aaadf0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization complete.\n",
      "Shape of X_train_tfidf: (17600, 807585)\n",
      "Shape of X_test_tfidf: (4400, 807585) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF ja est√† incl√≤s a `scikit-learn`, una de les llibreries m√©s\n",
    "# populars per a machine learning i el podem fer servir aix√≠ en el\n",
    "# nostre cas:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "vectorizer.fit(X_train_ngrams)\n",
    "\n",
    "X_train_tfidf = vectorizer.transform(X_train_ngrams)\n",
    "X_test_tfidf = vectorizer.transform(X_test_ngrams)\n",
    "\n",
    "print(\"TF-IDF vectorization complete.\")\n",
    "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AVVFL4AUCIEf",
   "metadata": {
    "id": "AVVFL4AUCIEf"
   },
   "source": [
    "Les matrius que ha generat la cel¬∑la anterior estan guardats amb una estructura de python que es diu \"Compressed Sparse Row sparse matrix\".\n",
    "\n",
    "Pensa que tens una taula enorme de nombres, per√≤ la majoria s√≥n zeros. Guardar tots aquests zeros √©s una p√®rdua de mem√≤ria i de temps.\n",
    "\n",
    "Per aix√≤ en Python (amb scipy) sovint es fan servir matrius disperses (sparse matrices), i una de les m√©s habituals √©s el format Compressed Sparse Row (CSR).\n",
    "\n",
    "En lloc de guardar tots els elements de la matriu, una CSR nom√©s guarda:\n",
    "+ Els valors no zero\n",
    "+ La columna de cada valor no zero\n",
    "+ On comen√ßa i acaba cada fila dins d‚Äôaquestes llistes\n",
    "\n",
    "Per√≤ tu, com a usuari, no cal que et preocupis gaire de com ho fa per dins:\n",
    "la tractes gaireb√© com si fos una matriu de NumPy, per√≤ amb algunes difer√®ncies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160aa057",
   "metadata": {
    "id": "160aa057"
   },
   "source": [
    "### Tasca 4: Implementaci√≥ d'un classificador ingenu de Bayes\n",
    "\n",
    "Has d'entrenar un classificador Naive Bayes amb les caracter√≠stiques TF-IDF del conjunt d'entrenament i les seves etiquetes de llengua corresponents.\n",
    "\n",
    "El model calcula la probabilitat que un text $d$ pertanyi a una llengua $c$:\n",
    "\n",
    "$$P(c \\mid d) \\propto P(c) \\prod_{i=1}^{V} P(w_i \\mid c)^{\\, f_i}$$\n",
    "\n",
    "On:\n",
    "+ $P(c)$: probabilitat pr√®via de la classe (per exemple, % de textos d'una llengua; en el nostre cas totes les lleng√ºes tenen la mateixa probabilitat).\n",
    "+ $w_i$: bigrama i-√®ssim del vocabulari.\n",
    "+ $f_i$: nombre de vegades que el bigrama $w_i$ apareix al text $d$.\n",
    "+ $P(w_i \\mid c)$: probabilitat que el bigrama $w_i$ aparegui en textos de la classe $c$.\n",
    "+ $V$: mida del vocabulari.\n",
    "\n",
    "El classificador escull la classe amb probabilitat m√©s alta.\n",
    "\n",
    "#### Com es calcula $P(w_i \\mid c)$?\n",
    "\n",
    "Normalment es calcula amb un model multinomial amb Laplace *smoothing* (per evitar zeros):\n",
    "\n",
    "$$P(w_i \\mid c) =\n",
    "\\frac{N_{i,c} + 1}{\\sum_{j=1}^{V} N_{j,c} + V}$$\n",
    "\n",
    "On:\n",
    "+ $N_{i,c}$: nombre total de vegades que la paraula $w_i$ apareix en tots els documents de la classe c.\n",
    "+ $V$: nombre total de paraules diferents del vocabulari.\n",
    "\n",
    "Aquesta f√≥rmula ve directament de la distribuci√≥ multinomial, perqu√® compta freq√º√®ncies de paraules dins d‚Äôuna ‚Äúbossa‚Äù pr√≤pia de cada classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmvW91jfvi40",
   "metadata": {
    "id": "mmvW91jfvi40"
   },
   "source": [
    "Fes una implementaci√≥ teva del model multinomial, √©s a dir, de les funcions que creen el model i apliquen el model al test:\n",
    "\n",
    " `MultinomialNBfit(X_train_tfidf, y_train))`\n",
    "\n",
    " i\n",
    "\n",
    " `MultinomialNBpredict(model, X_test_tfidf, y_test))`\n",
    "\n",
    "Imprimeix quina `accuracy` obtens. Si tot funciona correctament, hauries d'aconseguir una *accuracy* per sobre el 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c920a1-764c-46f0-b481-03f744ce1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "class CustomMultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior_ = None   # Log P(c)\n",
    "        self.feature_log_prob_ = None  # Log P(w|c)\n",
    "        self.classes_ = None           # Noms dels idiomes\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: Matriu dispersa (scipy.sparse) de (n_docs, n_features)\n",
    "        y: Array o llista amb les etiquetes (n_docs)\n",
    "        \"\"\"\n",
    "        # 1. Identificar classes √∫niques\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Inicialitzem les matrius amb float32 per estalviar RAM\n",
    "        self.class_log_prior_ = np.zeros(n_classes, dtype=np.float32)\n",
    "        self.feature_log_prob_ = np.zeros((n_classes, n_features), dtype=np.float32)\n",
    "\n",
    "        # 2. Iterar per cada idioma (classe)\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            # M√†scara: On s√≥n els documents d'aquest idioma 'c'?\n",
    "            # Aix√≤ crea un array boole√†\n",
    "            mask = (y == c)\n",
    "            \n",
    "            # Comptem quants docs tenim d'aquest idioma\n",
    "            n_docs_c = np.sum(mask)\n",
    "            \n",
    "            # P(c) = docs_idioma / docs_totals\n",
    "            # Guardem el LOG directament\n",
    "            self.class_log_prior_[idx] = np.log(n_docs_c / len(y))\n",
    "\n",
    "            # 3. Sumar els bigrames per aquest idioma\n",
    "            # X[mask, :] selecciona files de l'idioma. .sum(axis=0) suma columnes.\n",
    "            # Aix√≤ ens d√≥na quantes vegades surt cada bigrama en aquest idioma.\n",
    "            word_counts = X[mask, :].sum(axis=0) # Retorna (1, n_features)\n",
    "            \n",
    "            # Convertim a array pla i float32\n",
    "            word_counts = np.array(word_counts, dtype=np.float32).flatten()\n",
    "\n",
    "            # 4. Laplace Smoothing (+ alpha)\n",
    "            smoothed_counts = word_counts + self.alpha\n",
    "            \n",
    "            # Sumatori total de paraules en aquest idioma (+ smoothing del vocabulari)\n",
    "            smoothed_total_counts = smoothed_counts.sum()\n",
    "            \n",
    "            # 5. C√†lcul de Log Probabilitats condicionals\n",
    "            # log(counts) - log(total) √©s el mateix que log(counts/total)\n",
    "            self.feature_log_prob_[idx, :] = np.log(smoothed_counts) - np.log(smoothed_total_counts)\n",
    "\n",
    "        # Retornem self per convenci√≥ (fluent interface)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Utilitza √†lgebra lineal per calcular-ho tot de cop.\n",
    "        \"\"\"\n",
    "        # 1. C√†lcul de la probabilitat conjunta (Log Likelihood)\n",
    "        # F√≥rmula: X @ W.T + Priors\n",
    "        # X: (n_docs, n_features)\n",
    "        # feature_log_prob_.T: (n_features, n_classes)\n",
    "        # Resultat: (n_docs, n_classes) -> Puntuaci√≥ per cada idioma\n",
    "        \n",
    "        # L'operador @ gestiona eficientment sparse matrix * dense matrix\n",
    "        scores = X @ self.feature_log_prob_.T + self.class_log_prior_\n",
    "\n",
    "        # 2. Argmax: Quin √≠ndex t√© la puntuaci√≥ m√©s alta?\n",
    "        indices = np.argmax(scores, axis=1)\n",
    "\n",
    "        # 3. Mapejar √≠ndexs a noms d'idiomes\n",
    "        return self.classes_[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4750bc36",
   "metadata": {
    "id": "4750bc36"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (645594889.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    accuracy_score =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultinomialNB\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # ELS UNIFICAREM EN EL .py\n",
    "        self.alpha=alpha\n",
    "        self.languages=None\n",
    "        self.Pprior=None\n",
    "        self.Pconditional=None\n",
    "        return\n",
    "\n",
    "    def do_model():\n",
    "\n",
    "        self.MultinomialNBfit()\n",
    "        self.MultinomialNBpredict()\n",
    "\n",
    "    def MultinomialNBfit(self, X, y):\n",
    "            \"\"\"\n",
    "            Entrena el classificador Naive Bayes. Laplace Smoothing\n",
    "    \n",
    "            Args:\n",
    "                X (sparse matrix): Matriu de caracter√≠stiques (e.g., TF-IDF) d'entrenament.\n",
    "                y (array-like): Etiquetes de classe per a cada mostra.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            # en aquests ifs crec que no es gestiona el fet que hi hagi un nou idioma\n",
    "            # Crec que si nomes fessim el unique, i miressim lengths, podriem gestionar-ho\n",
    "            if self.languages == None:\n",
    "                self.languages = np.unique(y)\n",
    "            num_lang = len(self.languages)\n",
    "\n",
    "            # Aqui obtenim V (Ngrames totals)\n",
    "            num_ngram = X.shape[1]\n",
    "            denominador = 0\n",
    "\n",
    "            if self.Pprior == None:\n",
    "                self.Pprior = np.zeros(n_classes, dtype=np.float32)\n",
    "            if self.Pconditional == None:    \n",
    "                self.Pconditional = np.zeros((n_classes, n_features), dtype=np.float32)\n",
    "\n",
    "            # Enumerem pq a part de voler la llengua ('cat') volem saber\n",
    "            # La posivi√≥ que es troba per asignar una probabilitat\n",
    "            for index, language in enumerate(self.languages):\n",
    "                # Agafem matriu del idioma actual\n",
    "                actual_lang_mask = (y==language)\n",
    "\n",
    "                # n-docs del idoma actual\n",
    "                n_docs_actual_lang = np.sum(mask)\n",
    "                self.Pprior[index] = np.log(n_docs / len(y))\n",
    "\n",
    "                #Sumem el total de bigrames que hi ha d'aquell idioma\n",
    "                # X[mask, :] selecciona files de l'idioma. .sum(axis=0) suma columnes (ja que files √©s la probabilitat).\n",
    "                # Aix√≤ ens d√≥na quantes vegades surt cada bigrama en aquest idioma.\n",
    "                n_ngram_actual_lang = X[mask, :].sum(axis=0) # Retorna (1, n_features)\n",
    "                # com que no ens interessa (1, n_features)\n",
    "                # Ho convertim en array\n",
    "                n_ngram_actual_lang = np.array(n_ngram_actual_lang).flatten()\n",
    "\n",
    "                total_words_lang = n_ngram_actual_lang.sum()\n",
    "                denominador = np.log(total_words_lang + num_ngram)\n",
    "\n",
    "                numerador = np.log( total_words_lang + 1)\n",
    "\n",
    "                self.Pconditional[index, :] = numerador - denominador\n",
    "                \n",
    "             #Ells posaven aix√≤   \n",
    "            #return model\n",
    "            return self\n",
    "    \n",
    "    def MultinomialNBpredict(model,X, y):\n",
    "            \"\"\"\n",
    "            Aplica el classificador Naive Bayes. \n",
    "            Fa la probabilitat de que un text (d) sigui de la llengua (c)\n",
    "            \n",
    "    \n",
    "            Args:\n",
    "                X (sparse matrix): Matriu de caracter√≠stiques (e.g., TF-IDF) de test.\n",
    "                y (array-like): Etiquetes de classe per a cada mostra del test.\n",
    "            \"\"\"\n",
    "\n",
    "        \n",
    "            \n",
    "            accuracy_score = \n",
    "        \n",
    "    \n",
    "        \n",
    "            return accuracy_score\n",
    "\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e69cd7-fc57-4715-a4aa-15953bd4b737",
   "metadata": {},
   "source": [
    "bans de donar el codi per bo, verifica aquests punts:\n",
    "\n",
    "    [ ] Tipus de Dades: Est√†s fent servir np.float32 a les matrius internes?\n",
    "\n",
    "    [ ] Sparse Aware: Al m√®tode fit, quan sumes els recomptes, ho fas sobre la matriu sparse directament (sense convertir-la a dense/array abans d'hora)?\n",
    "\n",
    "    [ ] Logaritmes: Est√†s guardant els logaritmes, no les probabilitats directes (per evitar underflow)?\n",
    "\n",
    "    [ ] Operador @: Al predict, fas servir multiplicaci√≥ matricial en lloc de bucles?\n",
    "\n",
    "    [ ] Laplace: Assegura't que sumes alpha tant al numerador com al denominador (multiplicat pel vocabulari al denominador impl√≠citament al fer la suma total del numerador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d4cdb-4c6e-480f-b61a-63d6a1001cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aniran a buscar eso a funcions implementades aqui o altres no implementades a skilearn\n",
    "\n",
    "Jugar amb els parametres del argmax i el smoothing i provar de petar algun idioma\n",
    "\n",
    "Si tinc model a preedir sempre angles que creus que pot ser que ho ocasioni (que √©s simplificat i facil)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
